# SE4050 ‚Äì Deep Learning  
### Reinforcement Learning Lab 1  

**Name:** Oshada Navindra 
**Student ID:**  IT22079022    
**Degree Program:** BSc (Hons) in Information Technology ‚Äì Information Technology Specialization  
**Year:** 4  

---

## üìå Objective
The objective of this lab is to gain coding experience with two Reinforcement Learning approaches:  

1. **Markov Decision Process (MDP)**  
   - Policy Iteration / Value Iteration (Model-Based)  

2. **Q-Learning**  
   - Learning optimal policy from interactions (Model-Free)  

---

## üìù Tasks

### **Question 1: MDP and Q-Learning**
- Uploaded and completed the provided notebooks:  
  - `Markov_Decision_Process.ipynb`  
  - `GridWorld_Qlearning.ipynb`  
- Filled in the missing code sections (`# type your code here`).  
- Increased the GridWorld size to observe execution time and convergence changes.  
- Added screenshots of the completed parts at the end of each notebook.  

### **Question 2: Model-Based vs Model-Free**
- Modified the MDP notebook to compare execution time and convergence between:  
  - **Model-Based (Policy Iteration / Value Iteration)**  
  - **Model-Free (Q-Learning)**  
- Added plots and screenshots of results.  
- Wrote a short explanation of the difference between the two approaches.  

---

## üìä Results Summary

- **Random Agent**: Performance inconsistent, no learning.  
- **Q-Learning Agent**: Gradually improved and converged to near-optimal policy.  
- **Policy Iteration / Value Iteration**: Faster convergence due to full knowledge of model.  
- **Q-Learning vs MDP**:  
  - Model-Based = faster, requires environment model.  
  - Model-Free = slower, works without knowing model.  

---

## üìÇ Repository Structure
